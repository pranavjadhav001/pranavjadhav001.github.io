---
layout: inner
position: left
title: 'Implementing Attention Layer'
date: 2022-01-21 01:00:00
categories: development
tags: Transformers Tensorflow Attention
featured_image: '/img/posts/medium-attention.png'
project_link: 'https://medium.com/@pranavjadhav001/implementing-multi-head-self-attention-layer-using-tensorflow-e19c8fc7887'
button_icon: 'medium'
button_text: 'Visit medium'
lead_text: 'article is about how I implemented Multi-Head Self-Attention module'
---
