---
layout: inner
position: left
title: 'Multi-Head Self-Attention Layer'
date: 2022-01-20 01:00:00
categories: development
tags: Transformers Tensorflow Attention
featured_image: '/img/posts/mhsa.png'
project_link: 'https://medium.com/@pranavjadhav001/implementing-multi-head-self-attention-layer-using-tensorflow-e19c8fc7887'
button_icon: 'medium'
button_text: 'Visit medium'
lead_text: 'Implementing Multi-Head Self-Attention Layer using TensorFlow'
---